<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dynamic TensorFlow.js Prediction</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>
</head>
<body>
    <h1>Dynamic TensorFlow.js Prediction</h1>
    <div id="loading" style="display:none;">Loading model, please wait...</div>
    <div id="error" style="display:none; color: red;"></div>
    <video id="webcam" autoplay width="640" height="480"></video>
    <div id="prediction" style="color: black; background-color: white; font-size: 20px;"></div>

    <script>
        document.addEventListener('DOMContentLoaded', async () => {
            const urlParams = new URLSearchParams(window.location.search);
            const modelType = urlParams.get('model') || 'model_jab.json';
            let modelPath, metadataPath;

            if (modelType === 'model_jab.json') {
                modelPath = `/static/model/${modelType}`;
                metadataPath = `/static/model/metadata.json`;  // Assuming metadata is in the same folder
            } else if (modelType === 'model_curl.json') {
                modelPath = `/static/model2/${modelType}`;
                metadataPath = `/static/model2/metadata.json`;
            } else {
                modelPath = `/static/model/model_jab.json`;
                metadataPath = `/static/model/metadata.json`;
            }
            
            console.log('Loading model and metadata from:', modelPath, metadataPath);
            document.getElementById('loading').style.display = 'block';

            try {
                const { model, metadata } = await loadModelAndMetadata(modelPath, metadataPath);
                console.log('Model and metadata loaded');
                document.getElementById('loading').style.display = 'none';
                await startWebcam();
                await predictLoop(model, metadata);
            } catch (error) {
                console.error('Error loading the model or metadata:', error);
                document.getElementById('loading').style.display = 'none';
                document.getElementById('error').textContent = `Error loading model: ${error.message}`;
                document.getElementById('error').style.display = 'block';
            }
        });

        async function loadModelAndMetadata(modelUrl, metadataUrl) {
            const [model, metadataResponse] = await Promise.all([
                tf.loadLayersModel(modelUrl),
                fetch(metadataUrl)
            ]);
            const metadata = await metadataResponse.json();
            console.log("Loaded metadata:", metadata);  // Confirm metadata is loaded correctly
            return { model, metadata };
        }


        async function startWebcam() {
            try {
                const video = document.getElementById('webcam');
                const stream = await navigator.mediaDevices.getUserMedia({ video: true });
                video.srcObject = stream;
                await new Promise(resolve => video.onloadedmetadata = resolve);
                console.log("Webcam successfully started");
            } catch (error) {
                console.error('Error accessing the webcam:', error);
                document.getElementById('error').textContent = `Webcam Error: ${error.message}`;
                document.getElementById('error').style.display = 'block';
            }
        }

        async function predictLoop(model, metadata) {
            const video = document.getElementById('webcam');
            while (true) {
                const prediction = await predict(video, model, metadata);
                console.log("Current prediction displayed:", prediction);  // Log the prediction being displayed
                document.getElementById('prediction').innerText = `Prediction: ${prediction}`;
                await tf.nextFrame();
            }
        }


        async function predict(videoElement, model, metadata) {
            return tf.tidy(() => {
                let tensor = tf.browser.fromPixels(videoElement)
                    .resizeNearestNeighbor([224, 224])  // Resize if necessary
                    .toFloat()
                    .div(tf.scalar(127.5))
                    .sub(tf.scalar(1))
                    .expandDims();

                // Reshape or pad the tensor to fit the expected input shape of the model
                let flattenedTensor = tensor.reshape([-1]); // Flatten the tensor
                if (flattenedTensor.shape[0] > 14739) {
                    // Slice the tensor to match the required input size
                    flattenedTensor = flattenedTensor.slice([0], [14739]);
                } else if (flattenedTensor.shape[0] < 14739) {
                    // Pad the tensor if it's smaller than required
                    const padding = 14739 - flattenedTensor.shape[0];
                    const paddings = [[0, padding]]; // pad end of the tensor
                    flattenedTensor = flattenedTensor.pad(paddings);
                }

                // Reshape to match the input shape of the model
                flattenedTensor = flattenedTensor.reshape([1, 14739]);

                const predictions = model.predict(flattenedTensor).dataSync();
                if (metadata && metadata.labels) {
                    const predictedIndex = predictions.indexOf(Math.max(...predictions));
                    return metadata.labels[predictedIndex];
                }
                return predictions.map(p => p.toFixed(3)).join(', ');
            });
        }



    </script>
</body>
</html>
