<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dynamic TensorFlow.js Prediction</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>
</head>
<body>
    <h1>Dynamic TensorFlow.js Prediction</h1>
    <div id="loading" style="display:none;">Loading model, please wait...</div>
    <div id="error" style="display:none; color: red;"></div>
    <video id="webcam" autoplay width="640" height="480"></video>
    <div id="prediction" style="color: black; background-color: white; font-size: 20px;"></div>

    <script>
        document.addEventListener('DOMContentLoaded', async () => {
            const urlParams = new URLSearchParams(window.location.search);
            const modelType = urlParams.get('model') || 'model_jab.json';
            let modelPath, metadataPath;

            if (modelType === 'model_jab.json') {
                modelPath = `/static/model/${modelType}`;
                metadataPath = `/static/model/metadata.json`;  // Assuming metadata is in the same folder
            } else if (modelType === 'model_curl.json') {
                modelPath = `/static/model2/${modelType}`;
                metadataPath = `/static/model2/metadata.json`;
            } else {
                modelPath = `/static/model/model_jab.json`;
                metadataPath = `/static/model/metadata.json`;
            }
            
            console.log('Loading model and metadata from:', modelPath, metadataPath);
            document.getElementById('loading').style.display = 'block';

            try {
                const { model, metadata } = await loadModelAndMetadata(modelPath, metadataPath);
                console.log('Model and metadata loaded');
                document.getElementById('loading').style.display = 'none';
                await startWebcam();
                await predictLoop(model, metadata);
            } catch (error) {
                console.error('Error loading the model or metadata:', error);
                document.getElementById('loading').style.display = 'none';
                document.getElementById('error').textContent = `Error loading model: ${error.message}`;
                document.getElementById('error').style.display = 'block';
            }
        });

        async function loadModelAndMetadata(modelUrl, metadataUrl) {
            const [model, metadataResponse] = await Promise.all([
                tf.loadLayersModel(modelUrl),
                fetch(metadataUrl)
            ]);
            const metadata = await metadataResponse.json();
            console.log("Loaded metadata:", metadata);  // Confirm metadata is loaded correctly
            return { model, metadata };
        }


        async function startWebcam() {
            try {
                const video = document.getElementById('webcam');
                const stream = await navigator.mediaDevices.getUserMedia({ video: true });
                video.srcObject = stream;
                await new Promise(resolve => video.onloadedmetadata = resolve);
                console.log("Webcam successfully started");
            } catch (error) {
                console.error('Error accessing the webcam:', error);
                document.getElementById('error').textContent = `Webcam Error: ${error.message}`;
                document.getElementById('error').style.display = 'block';
            }
        }

        async function predictLoop(model, metadata) {
            const video = document.getElementById('webcam');
            while (true) {
                const prediction = await predict(video, model, metadata);
                console.log("Current prediction displayed:", prediction);  // Log the prediction being displayed
                document.getElementById('prediction').innerText = `Prediction: ${prediction}`;
                await tf.nextFrame();
            }
        }


        async function predict(videoElement, model, metadata) {
            return tf.tidy(() => {
                let tensor = tf.browser.fromPixels(videoElement)
                    .resizeNearestNeighbor([224, 224])  // Assuming the model was trained with this input size
                    .toFloat()
                    .div(tf.scalar(127.5))
                    .sub(tf.scalar(1))
                    .expandDims();

                // Check if we need to flatten the tensor for the model
                if (model.inputs[0].shape.length === 2) {
                    // Flatten the tensor if the model expects a 2D input
                    tensor = tensor.reshape([1, -1]);  // This reshapes it to [batch size, features]
                }

                const predictions = model.predict(tensor).dataSync();
                if (metadata && metadata.labels) {
                    const predictedIndex = predictions.indexOf(Math.max(...predictions));
                    return metadata.labels[predictedIndex];
                }
                return predictions.map(p => p.toFixed(3)).join(', ');
            });
        }

    </script>
</body>
</html>
